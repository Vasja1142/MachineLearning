{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: ELU, Mean abs grad: 0.4905975059140474\n",
      "Activation: Hardtanh, Mean abs grad: 0.3392139330675127\n",
      "Activation: LeakyReLU, Mean abs grad: 0.33811296799097496\n",
      "Activation: LogSigmoid, Mean abs grad: 0.23797560934035572\n",
      "Activation: PReLU, Mean abs grad: 0.3511333481586917\n",
      "Activation: ReLU, Mean abs grad: 0.3519847784793819\n",
      "Activation: ReLU6, Mean abs grad: 0.42508748886015385\n",
      "Activation: RReLU, Mean abs grad: 0.387764573182867\n",
      "Activation: SELU, Mean abs grad: 0.5944356032274664\n",
      "Activation: CELU, Mean abs grad: 0.48986828326713294\n",
      "Activation: Sigmoid, Mean abs grad: 0.0071115260634798\n",
      "Activation: Softplus, Mean abs grad: 0.33270451071934076\n",
      "Activation: Softshrink, Mean abs grad: 0.24824757158756255\n",
      "Activation: Softsign, Mean abs grad: 0.06396905900910496\n",
      "Activation: Tanh, Mean abs grad: 0.17454524965025484\n",
      "Activation: Tanhshrink, Mean abs grad: 0.027310636688192523\n",
      "Activation: Hardshrink, Mean abs grad: 0.6592802187800407\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "seed = int(input())  # Не забудьте ввести число при запуске!\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "NUMBER_OF_EXPERIMENTS = 200\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.fc1 = torch.nn.Linear(1, 1, bias=False)\n",
    "        self.fc1.weight.data.fill_(1.)\n",
    "        self.fc2 = torch.nn.Linear(1, 1, bias=False)\n",
    "        self.fc2.weight.data.fill_(1.)\n",
    "        self.fc3 = torch.nn.Linear(1, 1, bias=False)\n",
    "        self.fc3.weight.data.fill_(1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def get_fc1_grad_abs_value(self):\n",
    "        return torch.abs(self.fc1.weight.grad)\n",
    "\n",
    "def get_fc1_grad_abs_value(net, x):\n",
    "    output = net.forward(x)\n",
    "    output.backward()\n",
    "    fc1_grad = net.get_fc1_grad_abs_value().item()\n",
    "    net.zero_grad()\n",
    "    return fc1_grad\n",
    "\n",
    "activations = {'ELU': torch.nn.ELU(), 'Hardtanh': torch.nn.Hardtanh(),\n",
    "               'LeakyReLU': torch.nn.LeakyReLU(), 'LogSigmoid': torch.nn.LogSigmoid(),\n",
    "               'PReLU': torch.nn.PReLU(), 'ReLU': torch.nn.ReLU(), 'ReLU6': torch.nn.ReLU6(),\n",
    "               'RReLU': torch.nn.RReLU(), 'SELU': torch.nn.SELU(), 'CELU': torch.nn.CELU(),\n",
    "               'Sigmoid': torch.nn.Sigmoid(), 'Softplus': torch.nn.Softplus(),\n",
    "               'Softshrink': torch.nn.Softshrink(), 'Softsign': torch.nn.Softsign(),\n",
    "               'Tanh': torch.nn.Tanh(), 'Tanhshrink': torch.nn.Tanhshrink(),\n",
    "               'Hardshrink': torch.nn.Hardshrink()}\n",
    "\n",
    "for name, activation in activations.items():  # Итерируемся по парам (имя, объект активации)\n",
    "    net = SimpleNet(activation=activation)  # Создаём новую сеть для каждой активации\n",
    "\n",
    "    fc1_grads = []\n",
    "    for x in torch.randn((NUMBER_OF_EXPERIMENTS, 1)):\n",
    "        fc1_grads.append(get_fc1_grad_abs_value(net, x))\n",
    "\n",
    "    print(f\"Activation: {name}, Mean abs grad: {np.mean(fc1_grads)}\") # Выводим для каждой активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.010186273460276426\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "seed = int(input())\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "NUMBER_OF_EXPERIMENTS = 200\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.fc1 = torch.nn.Linear(1, 1, bias=False)  # one neuron without bias\n",
    "        self.fc1.weight.data.fill_(1.)  # init weight with 1\n",
    "        self.fc2 = torch.nn.Linear(1, 1, bias=False)\n",
    "        self.fc2.weight.data.fill_(1.)\n",
    "        self.fc3 = torch.nn.Linear(1, 1, bias=False)\n",
    "        self.fc3.weight.data.fill_(1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def get_fc1_grad(self):\n",
    "        return self.fc1.weight.grad\n",
    "\n",
    "activation =  torch.nn.Tanh()\n",
    "\n",
    "net = SimpleNet(activation=activation)\n",
    "\n",
    "fc1_grads_sum = 0.0  # Сумма градиентов\n",
    "for x in torch.randn((NUMBER_OF_EXPERIMENTS, 1)):\n",
    "    output = net(x)\n",
    "    output.backward()\n",
    "    fc1_grads_sum += net.get_fc1_grad().item()  # Добавляем ГРАДИЕНТ (не абсолютное значение)\n",
    "    net.zero_grad()\n",
    "\n",
    "print(fc1_grads_sum / NUMBER_OF_EXPERIMENTS) #среднее значение градиентов\n",
    "# print(abs(fc1_grads_sum / NUMBER_OF_EXPERIMENTS)) # модуль среднего"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
